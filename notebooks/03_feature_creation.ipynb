{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5056005",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m lags_elegidos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m28\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Lista de procesos y la operación aplicada (código seguido de criterio):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# i) Imputación de variables numéricas (sales & lag features)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m num_imputer \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleImputer\u001b[49m(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales_lag_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m lags_elegidos] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroll_mean_7\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroll_mean_28\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Imputación global como ejemplo (en producción: por grupo sería mejor)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "lags_elegidos = [1, 7, 14, 28]\n",
    "# Lista de procesos y la operación aplicada (código seguido de criterio):\n",
    "\n",
    "# i) Imputación de variables numéricas (sales & lag features)\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "num_cols = ['sales'] + [f'sales_lag_{lag}' for lag in lags_elegidos] + ['roll_mean_7','roll_mean_28']\n",
    "\n",
    "# Imputación global como ejemplo (en producción: por grupo sería mejor)\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "# ii) Imputación de variables categóricas (store)\n",
    "df['store'] = df['store'].cat.add_categories(['unknown']).fillna('unknown')\n",
    "\n",
    "# iii) Codificación de variables categóricas\n",
    "item_freq = df['item'].value_counts(normalize=True)\n",
    "df['item_freq_enc'] = df['item'].map(item_freq)\n",
    "\n",
    "# One-hot para store verificar si se cambia por otro tipo de categorización\n",
    "top_stores = df['store'].value_counts().nlargest(10).index.tolist()\n",
    "for s in top_stores:\n",
    "    df[f'store_is_{s}'] = (df['store'] == s).astype(int)\n",
    "\n",
    "# iv) Tratamiento de outliers (sales)\n",
    "Q1 = df['sales'].quantile(0.25)\n",
    "Q3 = df['sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "df['sales_clipped'] = df['sales'].clip(lower=lower, upper=upper)\n",
    "\n",
    "# v) Transformación de variables numéricas\n",
    "df['sales_log1p'] = np.log1p(df['sales_clipped'])\n",
    "\n",
    "# vi) Escalado de características\n",
    "scale_cols = ['item_freq_enc','sales_lag_1','sales_lag_7','roll_mean_7','sales_log1p']\n",
    "scaler = StandardScaler()\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "\n",
    "print('Transformaciones aplicadas. Muestra:')\n",
    "print(df[scale_cols + ['sales']].head())\n",
    "\n",
    "# Guardar dataset transformado para siguiente etapa (pipeline/operadores)\n",
    "out_cols = ['date','store','item','sales'] + [f'sales_lag_{lag}' for lag in lags_elegidos] + ['roll_mean_7','roll_mean_28','item_freq_enc','sales_clipped','sales_log1p'] + [f'store_is_{s}' for s in top_stores]\n",
    "\n",
    "df_out = df[out_cols].copy()\n",
    "df_out.to_parquet('train_feature_exploration.parquet', index=False)\n",
    "print('\\nGuardado: train_feature_exploration.parquet con shape', df_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25cf72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'operators'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfeature_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountFrequencyEncoder\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moperators\u001b[39;00m  \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'operators'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import operators  \n",
    "\n",
    "\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/raw/train.csv', parse_dates=['date'])\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Imputador categórico sencillo:\n",
    "    - Rellena NaNs con un string (por defecto 'Missing').\n",
    "    - No revisa tipos, solo hace fillna().\n",
    "    \"\"\"\n",
    "    def __init__(self, variables, fill_value=\"Missing\"):\n",
    "        self.variables = variables\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # no aprende nada, solo dejamos el objeto listo\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for var in self.variables:\n",
    "            if var in X.columns:\n",
    "                X[var] = X[var].fillna(self.fill_value).astype(str)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crear variables de calendario a partir de `date`\n",
    "data_train['date'] = pd.to_datetime(data_train['date'])\n",
    "\n",
    "data_train['year']  = data_train['date'].dt.year\n",
    "data_train['month'] = data_train['date'].dt.month\n",
    "data_train['day_of_week_name'] = data_train['date'].dt.day_name()\n",
    "\n",
    "# tratamos store e item como categóricas\n",
    "data_train['store'] = data_train['store'].astype('O')\n",
    "data_train['item']  = data_train['item'].astype('O')\n",
    "\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Definición de variables\n",
    "\n",
    "TARGET = \"sales\"\n",
    "\n",
    "FEATURES = [\"store\", \"item\", \"year\", \"month\", \"day_of_week_name\"]\n",
    "\n",
    "# categóricas\n",
    "CATEGORICAL_VARS = [\"store\", \"item\", \"day_of_week_name\"]\n",
    "CATEGORICAL_VARS_IMPUTE = [\"store\", \"item\"]   # para imputador categórico\n",
    "CATEGORICAL_VARS_FREQ   = [\"store\", \"item\"]   # para CountFrequencyEncoder\n",
    "\n",
    "# numéricas\n",
    "NUMERICAL_VARS = [\"year\", \"month\"]\n",
    "\n",
    "# mapping para día de la semana -> número\n",
    "DAY_OF_WEEK_MAPPING = {\n",
    "    \"Monday\": 0,\n",
    "    \"Tuesday\": 1,\n",
    "    \"Wednesday\": 2,\n",
    "    \"Thursday\": 3,\n",
    "    \"Friday\": 4,\n",
    "    \"Saturday\": 5,\n",
    "    \"Sunday\": 6\n",
    "}\n",
    "\n",
    "# Aseguramos tipos categóricos (similar al ejemplo de MSSubClass, GarageCars, etc.)\n",
    "for col in CATEGORICAL_VARS:\n",
    "    data_train[col] = data_train[col].astype(\"O\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train-Test Split\n",
    "\n",
    "X = data_train[FEATURES].copy()\n",
    "y = data_train[TARGET].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=True, random_state=2025\n",
    ")\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Construcción del Pipeline de ingeniería de características\n",
    "\n",
    "sales_feature_pipeline = Pipeline([\n",
    "\n",
    "    # 1. Imputación de variables categóricas (store, item)\n",
    "    (\"cat_missing_imputation\", SimpleCategoricalImputer(\n",
    "        variables=CATEGORICAL_VARS_IMPUTE,\n",
    "        fill_value=\"Missing\"\n",
    "    )),\n",
    "\n",
    "    # 2. Imputación de variables numéricas (year, month) con mediana\n",
    "    (\"num_median_imputation\", MeanMedianImputer(\n",
    "        imputation_method=\"median\",\n",
    "        variables=NUMERICAL_VARS\n",
    "    )),\n",
    "\n",
    "    # 3. Codificación de variables categóricas store e item por frecuencia\n",
    "    (\"cat_freq_encoder\", CountFrequencyEncoder(\n",
    "        encoding_method=\"frequency\",\n",
    "        variables=CATEGORICAL_VARS_FREQ\n",
    "    )),\n",
    "\n",
    "    # 4. Mapeo de variable categórica ordinal (día de la semana) con Mapper\n",
    "    (\"dayofweek_mapper\", operators.Mapper(\n",
    "        mappings=DAY_OF_WEEK_MAPPING,\n",
    "        variables=[\"day_of_week_name\"]\n",
    "    )),\n",
    "\n",
    "    # 5. Normalización de variables (MinMaxScaler)\n",
    "    (\"feature_scaler\", MinMaxScaler())\n",
    "\n",
    "])\n",
    "\n",
    "sales_feature_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Aplicamos preprocesamiento a los datos\n",
    "\n",
    "# Ajustar pipeline\n",
    "sales_feature_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Transformar datos de entrenamiento\n",
    "X_train_transformed = sales_feature_pipeline.transform(X_train)\n",
    "\n",
    "# Lo pasamos a DataFrame solo para inspección / guardado\n",
    "X_train_transformed_df = pd.DataFrame(\n",
    "    X_train_transformed,\n",
    "    columns=FEATURES\n",
    ")\n",
    "\n",
    "X_train_transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Guardar datos procesados y pipeline\n",
    "\n",
    "# Guardamos datos transformados\n",
    "os.makedirs(\"../data/interim\", exist_ok=True)\n",
    "preproc_train_path = \"../data/interim/preproc_train.csv\"\n",
    "Xy_train_preproc = pd.concat(\n",
    "    [X_train_transformed_df.reset_index(drop=True),\n",
    "     y_train.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "Xy_train_preproc.to_csv(preproc_train_path, index=False)\n",
    "\n",
    "print(f\"Datos de entrenamiento preprocesados guardados en: {preproc_train_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_path = \"../models/feature_engineering_pipeline.pkl\"\n",
    "joblib.dump(sales_feature_pipeline, pipeline_path)\n",
    "\n",
    "print(f\"Pipeline de ingeniería de características guardado en: {pipeline_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
