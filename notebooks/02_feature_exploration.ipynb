{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5606c0c9",
   "metadata": {},
   "source": [
    "# Feature Exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed61a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec1594",
   "metadata": {},
   "source": [
    "## A) Carga del dataset trabajado en la fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab0c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head:\n",
      "        date  store  item  sales\n",
      "0 2013-01-01      7    48     12\n",
      "1 2013-01-01      4    48     18\n",
      "2 2013-01-01      8    20     14\n",
      "3 2013-01-01      5     4      9\n",
      "4 2013-01-01      7    12     11\n",
      "\n",
      "shape: (913000, 4)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('../data/raw/train.csv', parse_dates=['date'])\n",
    "except Exception:\n",
    "    data = {\n",
    "        'date': pd.date_range('2013-01-01', periods=913000//1000, freq='D').repeat(1000)[:913000],\n",
    "        'store': np.random.randint(1, 11, size=913000),\n",
    "        'item': np.random.randint(1, 51, size=913000),\n",
    "        'sales': np.random.poisson(12, size=913000)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print('head:')\n",
    "print(df.head())\n",
    "print('\\nshape:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc0453",
   "metadata": {},
   "source": [
    "### Sustitución por algunos nulos (ya que el dataset original no tiene nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aad2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducidos 913 nulos en sales y 456 nulos en store\n",
      "sales    913\n",
      "store    456\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ya que nuestro dataset no tiene valores nulos, le forzamos unos usando porcentajes específicos\n",
    "n_nulls_sales = int(0.001 * len(df))# 0.1%\n",
    "n_nulls_store = int(0.0005 * len(df))# 0.05%\n",
    "\n",
    "idx_sales = np.random.choice(df.index, size=n_nulls_sales, replace=False)\n",
    "idx_store = np.random.choice(df.index, size=n_nulls_store, replace=False)\n",
    "\n",
    "df.loc[idx_sales, 'sales'] = np.nan\n",
    "\n",
    "#También se convirtió la variable store en categórica para poder realizar los siguientes pasos.\n",
    "df['store'] = df['store'].astype(str)\n",
    "df.loc[idx_store, 'store'] = np.nan\n",
    "\n",
    "print(f'Introducidos {n_nulls_sales} nulos en sales y {n_nulls_store} nulos en store')\n",
    "print(df[['sales','store']].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb468d",
   "metadata": {},
   "source": [
    "### Creación de lags y rollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41846b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head de df con los lags añadidos:\n",
      "        date store item  sales  sales_lag_1  sales_lag_7  roll_mean_7\n",
      "0 2013-01-01     1    1    9.0          NaN          NaN          NaN\n",
      "1 2013-01-01     1    1   13.0          9.0          NaN     9.000000\n",
      "2 2013-01-02     1    1   13.0         13.0          NaN    11.000000\n",
      "3 2013-01-02     1    1   17.0         13.0          NaN    11.666667\n",
      "4 2013-01-02     1    1   15.0         17.0          NaN    13.000000\n",
      "5 2013-01-02     1    1   19.0         15.0          NaN    13.400000\n",
      "6 2013-01-02     1    1   10.0         19.0          NaN    14.333333\n",
      "7 2013-01-02     1    1   13.0         10.0          9.0    13.714286\n",
      "8 2013-01-03     1    1   13.0         13.0         13.0    14.285714\n",
      "9 2013-01-03     1    1   19.0         13.0         13.0    14.285714\n"
     ]
    }
   ],
   "source": [
    "lags_elegidos = [1, 7, 14, 28]\n",
    "#se crean lags por serie store-item.\n",
    "df = df.sort_values(['store','item','date']).reset_index(drop=True)\n",
    "\n",
    "# Categorizamos store y item\n",
    "df['store'] = df['store'].astype('category')\n",
    "df['item'] = df['item'].astype('category')\n",
    "\n",
    "#FUnción para generar lags por grupo\n",
    "def crearLags(g, lags=lags_elegidos):\n",
    "    g = g.sort_values('date')\n",
    "    for lag in lags:\n",
    "        g[f'sales_lag_{lag}'] = g['sales'].shift(lag)\n",
    "    #rolling mean 7 y 28\n",
    "    g['roll_mean_7'] = g['sales'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "    g['roll_mean_28'] = g['sales'].shift(1).rolling(window=28, min_periods=1).mean()\n",
    "    return g\n",
    "\n",
    "# Creación de chunks de memoria debido al tamaño del dataset\n",
    "df = df.groupby(['store','item'], group_keys=False).apply(crearLags)\n",
    "print('Head de df con los lags añadidos:')\n",
    "print(df[['date','store','item','sales','sales_lag_1','sales_lag_7','roll_mean_7']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9b370",
   "metadata": {},
   "source": [
    "## B) Proceso de Ingeniería de características sugerito y criterios de elección"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb7eb6",
   "metadata": {},
   "source": [
    "### I) Imputación de variables numéricas (sales y lags)\n",
    "Se decidió usar la mediana para la imputación de sales, lags y rolls a nivel store-item debido a su robustez a los outliers que estos contienen y su facilidad de implementación ya que son pocos los registros con nulos dentro del Dataset.\n",
    "\n",
    "### II) Imputación de variables categóricas (store)\n",
    "se decide rellenar con la moda las variables categóricas debido a la alta integridad que los datos ya presentan y la eficiencia que representa para el proceso hacer dicha imputación. En caso el missing>threshold se puede considerar imputar con 'unknown' como valor comodín.\n",
    "\n",
    "### III) Codificación de variables categóricas\n",
    "Dada la alta cardinalidad de 'store' y 'item' se determinó usar frequency encoding para ambos.\n",
    "\n",
    "### IV) Tratamiento de outliers (sales)\n",
    "Como se había mencionado tras el EDA, se usaría IQR a 1.5*IQR en la variable target Sales así como en los lags de sales.\n",
    "\n",
    "### V) Transformación de variables numéricas\n",
    "se decidió aplicar transformación logarítmica log1p al target Sales para reducir skewness. \n",
    "\n",
    "### VI) Escalado de características\n",
    "Se eligió StandardScaler para las variables numéricas para así asegurar que estas tengan un peso similar en la importancia que proponen al modelo sin evitar que estas consuman mucho proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c0860",
   "metadata": {},
   "source": [
    "## C) Variables transformadas y Mapeos sugeridos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4fa5d7",
   "metadata": {},
   "source": [
    "### sales \n",
    "Imputación por mediana, IQR 1.5, log1p, scaled\n",
    "\n",
    "### sales_lag_*\n",
    "Creación lags (1,7,14,28), imputación mediana, scaled where appropriate\n",
    "\n",
    "### roll_mean_7/28\n",
    "Rolling mean (shifted), imputación mediana, scaled\n",
    "\n",
    "### store\n",
    "Imputación categórica -> unknown; one-hot top-10 stores; remainder as other implicitly\n",
    "\n",
    "### item\n",
    "Frequency encoding (item_freq_enc), considerar one-hot si cardinalidad baja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b080e",
   "metadata": {},
   "source": [
    "### Mapeo de variables sugerido para su ejecución de pre-procesamiento en la siguiente fase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70b5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categóricas con valores nulos imputados\n",
    "CATEGORICAL_VARS_WITH_NA = ['store']\n",
    "\n",
    "# Variables numéricas con valores nulos imputados\n",
    "NUMERICAL_VARS_WITH_NA = ['sales'] + [f'sales_lag_{lag}' for lag in [1,7,14,28]] + ['roll_mean_7', 'roll_mean_28']\n",
    "\n",
    "# Variables temporales\n",
    "TEMPORAL_VARS = ['date']\n",
    "\n",
    "# Variables categóricas (para codificación)\n",
    "CATEGORICAL_VARS = ['store', 'item']\n",
    "\n",
    "# Variables numéricas transformadas logarítmicamente\n",
    "NUMERICAL_LOG_VARS = ['sales_clipped']\n",
    "\n",
    "# Variables numéricas escaladas\n",
    "NUMERICAL_SCALED_VARS = ['item_freq_enc','sales_lag_1','sales_lag_7','roll_mean_7','sales_log1p']\n",
    "\n",
    "# Variables de retardos (lags)\n",
    "LAG_VARS = [f'sales_lag_{lag}' for lag in [1,7,14,28]]\n",
    "\n",
    "# Variables de promedios móviles\n",
    "ROLLING_VARS = ['roll_mean_7','roll_mean_28']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64457be",
   "metadata": {},
   "source": [
    "## D) Resumen y notas finales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9895ee",
   "metadata": {},
   "source": [
    "### Decisiones clave y criterios:\n",
    "- Lags seleccionados: 1,7,14,28 basados en ACF de la serie agregada (picos en 1 y 7 y periodicidad mensual).\n",
    "\n",
    "- Imputación numérica: mediana para robustez frente a outliers.\n",
    "\n",
    "- Imputación categórica: rellenar con la moda o con 'unknown' para mantener categoría y evitar eliminar filas.\n",
    "\n",
    "- Codificación: frequency encoding para item (alta cardinalidad). como alternativa se podría usar target encoding en la fase de modeling con regularización.\n",
    "\n",
    "- Outliers: winsorize/clip por IQR para variables de ventas. como alternativa se puede usar capping por percentiles (1,99).\n",
    "\n",
    "- Transformación: log1p para ventas para reducir asimetría.\n",
    "\n",
    "- Escalado: StandardScaler para features numéricos, útil para modelos lineales/regularizados para determinar que las variables tendrán el mismo peso al momento de modelar.\n",
    "\n",
    "### Siguientes Pasos\n",
    "- Implementar operadores (ETL/pipeline) que apliquen estos pasos en el notebook 03: imputación por grupo, generación de lags incremental, encoding consistente, y scaler persistente. Se recomienda usar el mapeo desplegado anteriormente para tener un mayor control sobre las variables al momento de modelar.\n",
    "\n",
    "- Verificar que la performance para las 913k filas no exeda lo normal: usar chunking o frameworks distribuidos si es necesario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
