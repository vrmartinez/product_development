# Product Development - PredicciÃ³n de Ventas

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Proyecto del Curso - Pipeline de MLOps para PredicciÃ³n de Ventas**

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un **pipeline completo de MLOps** para la predicciÃ³n de ventas utilizando tÃ©cnicas de machine learning. El sistema estÃ¡ diseÃ±ado siguiendo las mejores prÃ¡cticas de ciencia de datos e ingenierÃ­a de software, incluyendo:

- âœ… AnÃ¡lisis exploratorio de datos (EDA)
- âœ… IngenierÃ­a de caracterÃ­sticas automatizada
- âœ… Entrenamiento y selecciÃ³n de modelos
- âœ… Pipeline de inferencia reproducible
- âœ… Arquitectura modular y escalable

---

## ğŸ¯ Objetivo del Proyecto

Desarrollar un sistema de predicciÃ³n de ventas que permita:
1. Procesar datos histÃ³ricos de ventas por tienda y artÃ­culo
2. Generar caracterÃ­sticas predictivas automÃ¡ticamente
3. Entrenar y evaluar mÃºltiples modelos de machine learning
4. Producir predicciones confiables para la planificaciÃ³n de inventario

---

## ğŸ“ OrganizaciÃ³n del Proyecto

```
product_development/
â”‚
â”œâ”€â”€ ğŸ“„ LICENSE                 <- Licencia de cÃ³digo abierto (MIT)
â”œâ”€â”€ ğŸ“„ Makefile                <- Comandos Ãºtiles (make data, make train, etc.)
â”œâ”€â”€ ğŸ“„ README.md               <- DocumentaciÃ³n principal del proyecto
â”œâ”€â”€ ğŸ“„ pyproject.toml          <- ConfiguraciÃ³n del proyecto y dependencias
â”œâ”€â”€ ğŸ“„ environment.yml         <- Entorno conda con todas las dependencias
â”‚
â”œâ”€â”€ ğŸ“‚ data/                   <- Datos del proyecto
â”‚   â”œâ”€â”€ external/              <- Datos de fuentes externas
â”‚   â”œâ”€â”€ processed/             <- Datos procesados listos para modelado
â”‚   â”‚   â”œâ”€â”€ preproc_train.csv  <- Dataset preprocesado
â”‚   â”‚   â””â”€â”€ test_predictions.csv <- Predicciones generadas
â”‚   â””â”€â”€ raw/                   <- Datos originales (inmutables)
â”‚       â””â”€â”€ train.csv          <- Dataset de entrenamiento (date, store, item, sales)
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                   <- DocumentaciÃ³n adicional del proyecto
â”‚
â”œâ”€â”€ ğŸ“‚ models/                 <- Modelos entrenados serializados
â”‚   â”œâ”€â”€ feature_engineering_pipeline.pkl  <- Pipeline de ingenierÃ­a de caracterÃ­sticas
â”‚   â””â”€â”€ sales_pipeline.pkl                <- Pipeline completo de predicciÃ³n
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/              <- Jupyter notebooks del flujo de trabajo
â”‚   â”œâ”€â”€ 01_Data_Exploration.ipynb      <- EDA: anÃ¡lisis y visualizaciones
â”‚   â”œâ”€â”€ 02_feature_exploration.ipynb   <- ExploraciÃ³n de caracterÃ­sticas
â”‚   â”œâ”€â”€ 03_feature_creation.ipynb      <- CreaciÃ³n de features con sklearn
â”‚   â”œâ”€â”€ 04_model_tuning_training.ipynb <- Ajuste y entrenamiento de modelos
â”‚   â”œâ”€â”€ 05_inference_calculation.ipynb <- CÃ¡lculo de predicciones
â”‚   â””â”€â”€ operators.py                   <- Transformadores para notebooks
â”‚
â”œâ”€â”€ ğŸ“‚ references/             <- Diccionarios de datos y materiales de referencia
â”‚
â”œâ”€â”€ ğŸ“‚ reports/                <- Reportes y anÃ¡lisis generados
â”‚   â””â”€â”€ figures/               <- GrÃ¡ficos y figuras para reportes
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                  <- Pruebas unitarias
â”‚   â””â”€â”€ test_data.py           <- Tests de validaciÃ³n de datos
â”‚
â””â”€â”€ ğŸ“‚ product_development/    <- ğŸ“¦ CÃ³digo fuente del paquete
    â”‚
    â”œâ”€â”€ __init__.py            <- InicializaciÃ³n del mÃ³dulo Python
    â”œâ”€â”€ config.py              <- ConfiguraciÃ³n de rutas y constantes
    â”œâ”€â”€ dataset.py             <- Funciones de carga y preparaciÃ³n de datos
    â”œâ”€â”€ features.py            <- Pipeline de ingenierÃ­a de caracterÃ­sticas
    â”œâ”€â”€ plots.py               <- Funciones de visualizaciÃ³n
    â”œâ”€â”€ transformers.py        <- Transformadores personalizados de sklearn
    â”œâ”€â”€ run_pipeline.py        <- Script principal del pipeline MLOps
    â”‚
    â””â”€â”€ modeling/              <- SubmÃ³dulo de modelado
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ train.py           <- Entrenamiento y evaluaciÃ³n de modelos
        â””â”€â”€ predict.py         <- Inferencia y generaciÃ³n de predicciones
```

---

## ğŸ”„ Flujo de Trabajo

El proyecto sigue un flujo de trabajo estructurado en 5 etapas:

### 1ï¸âƒ£ ExploraciÃ³n de Datos
**Notebook:** `01_Data_Exploration.ipynb`

- AnÃ¡lisis exploratorio del dataset de ventas
- EstadÃ­sticas descriptivas por tienda y artÃ­culo
- Visualizaciones de series temporales
- IdentificaciÃ³n de patrones y tendencias

### 2ï¸âƒ£ ExploraciÃ³n de CaracterÃ­sticas
**Notebook:** `02_feature_exploration.ipynb`

- AnÃ¡lisis de correlaciones
- EvaluaciÃ³n de variables candidatas
- SelecciÃ³n de caracterÃ­sticas relevantes

### 3ï¸âƒ£ CreaciÃ³n de CaracterÃ­sticas
**Notebook:** `03_feature_creation.ipynb`

Pipeline de ingenierÃ­a de caracterÃ­sticas que incluye:
- ğŸ“Š **Features de Lag**: 1, 7, 14, 28 dÃ­as
- ğŸ“ˆ **Medias MÃ³viles**: ventanas de 7 y 28 dÃ­as
- ğŸ·ï¸ **CodificaciÃ³n de Frecuencia**: para tiendas e items
- ğŸ“… **Features Temporales**: aÃ±o, mes, dÃ­a de la semana
- âš–ï¸ **Escalado MinMax**: normalizaciÃ³n de caracterÃ­sticas

### 4ï¸âƒ£ Entrenamiento del Modelo
**Notebook:** `04_model_tuning_training.ipynb`

EvaluaciÃ³n de mÃºltiples algoritmos:
- RegresiÃ³n Lineal
- Random Forest
- Gradient Boosting
- Support Vector Regression (SVR)
- XGBoost

### 5ï¸âƒ£ Inferencia
**Notebook:** `05_inference_calculation.ipynb`

- Carga del pipeline entrenado
- GeneraciÃ³n de predicciones
- EvaluaciÃ³n de mÃ©tricas (RMSE)

---

## ğŸš€ InstalaciÃ³n

### Prerrequisitos
- Python 3.11
- Conda (recomendado) o pip

### OpciÃ³n 1: Usando Conda (Recomendado)

```bash
# Clonar el repositorio
git clone https://github.com/franciscogonzalez-gal/product_development.git
cd product_development

# Crear entorno conda desde environment.yml
conda env create -f environment.yml

# Activar entorno
conda activate product_development

# Instalar el paquete en modo desarrollo
pip install -e .
```

### OpciÃ³n 2: Usando pip

```bash
# Clonar el repositorio
git clone https://github.com/franciscogonzalez-gal/product_development.git
cd product_development

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar el paquete en modo desarrollo
pip install -e .
```

---

## ğŸ’» Uso

### Ejecutar el Pipeline Completo

```bash
# Ejecutar pipeline completo (entrenamiento + inferencia)
python -m product_development.run_pipeline

# Solo inferencia (usando modelo existente)
python -m product_development.run_pipeline --skip-training

# Especificar rutas personalizadas
python -m product_development.run_pipeline \
    --input-path data/raw/train.csv \
    --output-path data/processed/predictions.csv
```

### Opciones del Pipeline

| OpciÃ³n | DescripciÃ³n |
|--------|-------------|
| `--input-path`, `-i` | Ruta a los datos crudos de entrenamiento |
| `--output-path`, `-o` | Ruta para guardar las predicciones |
| `--skip-training`, `-s` | Omitir entrenamiento y usar pipeline existente |
| `--inference-data`, `-d` | Ruta a datos para inferencia |

### Usar como Biblioteca

```python
from product_development.dataset import load_raw_data, prepare_dataset
from product_development.features import build_feature_pipeline
from product_development.modeling.train import train_and_evaluate_models
from product_development.modeling.predict import load_and_predict

# Cargar y preparar datos
data = load_raw_data()
prepared_data = prepare_dataset(data)

# Generar predicciones con modelo existente
predictions = load_and_predict(prepared_data)
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| CategorÃ­a | TecnologÃ­as |
|-----------|-------------|
| **Lenguaje** | Python 3.11 |
| **ManipulaciÃ³n de Datos** | Pandas, NumPy |
| **Machine Learning** | Scikit-learn, XGBoost |
| **IngenierÃ­a de CaracterÃ­sticas** | Feature-engine |
| **VisualizaciÃ³n** | Matplotlib, Seaborn |
| **AnÃ¡lisis EstadÃ­stico** | Statsmodels |
| **CLI** | Typer |
| **Logging** | Loguru |
| **SerializaciÃ³n** | Joblib |

---

## ğŸ“Š Estructura de Datos

### Dataset de Entrada (`train.csv`)

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `date` | datetime | Fecha de la venta |
| `store` | int | Identificador de la tienda |
| `item` | int | Identificador del artÃ­culo |
| `sales` | int | Cantidad de ventas |

### CaracterÃ­sticas Generadas

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| `year` | AÃ±o extraÃ­do de la fecha |
| `month` | Mes extraÃ­do de la fecha |
| `day_of_week_name` | Nombre del dÃ­a de la semana |
| `store` (encoded) | Tienda codificada por frecuencia |
| `item` (encoded) | ArtÃ­culo codificado por frecuencia |

---

## ğŸ§ª Pruebas

```bash
# Ejecutar todas las pruebas
pytest tests/

# Ejecutar con cobertura
pytest tests/ --cov=product_development
```

---

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

El modelo se evalÃºa utilizando:
- **RMSE** (Root Mean Square Error): MÃ©trica principal de evaluaciÃ³n
- ComparaciÃ³n de predicciones vs valores reales

---

## ğŸ‘¥ Autores

- **Galileo Team** - Universidad Galileo

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ™ Agradecimientos

- [Cookiecutter Data Science](https://cookiecutter-data-science.drivendata.org/) por la plantilla del proyecto
- Universidad Galileo por el soporte acadÃ©mico

---

<p align="center">
  <i>Desarrollado con â¤ï¸ para el curso de Desarrollo de Producto</i>
</p>